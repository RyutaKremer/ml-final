{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUeH0T__MYrS"
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchensemble\n",
        "!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxlWmKpB2pGX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchensemble import FastGeometricClassifier, SnapshotEnsembleClassifier\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsUTeMdZs9HD"
      },
      "source": [
        "n_estimators = 10\n",
        "lr = 1e-1\n",
        "weight_decay = 5e-4\n",
        "momentum = 0.9\n",
        "epochs = 10 #200\n",
        "\n",
        "batch_size = 256 #16\n",
        "\n",
        "data_dir = './data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgGi6DmMv9Wa"
      },
      "source": [
        "def set_model_FGE():\n",
        "  # Choose the Ensemble Method\n",
        "  model = FastGeometricClassifier(\n",
        "      estimator=resnet18(),\n",
        "      # estimator_args={\"bresnet18lock\": BasicBlock, \"num_blocks\": [2, 2, 2, 2]},\n",
        "      n_estimators=n_estimators,\n",
        "      cuda=True,\n",
        "  )\n",
        "\n",
        "  # Set the Optimizer\n",
        "  model.set_optimizer(\n",
        "      \"SGD\", lr=lr, weight_decay=weight_decay, momentum=momentum\n",
        "  )\n",
        "\n",
        "  # Set the Scheduler\n",
        "  model.set_scheduler(\"CosineAnnealingLR\", T_max=epochs)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwRBgK21NgcR"
      },
      "source": [
        "# not modified yet\n",
        "def _adjust_lr_modeified(\n",
        "    self, optimizer, epoch, i, n_iters, cycle, alpha_1, alpha_2\n",
        "):\n",
        "    \"\"\"\n",
        "    Set the internal learning rate scheduler for fast geometric ensemble.\n",
        "    Please refer to the original paper for details.\n",
        "    \"\"\"\n",
        "\n",
        "    def scheduler(i):\n",
        "        t = ((epoch % cycle) + i) / cycle\n",
        "        '''\n",
        "        if t < 0.5:\n",
        "            return alpha_1 * (1.0 - 2.0 * t) + alpha_2 * 2.0 * t\n",
        "        else:\n",
        "            return alpha_1 * (2.0 * t - 1.0) + alpha_2 * (2.0 - 2.0 * t)\n",
        "        '''\n",
        "        r = (1 - (1 - 2 * t) ** 2) ** 0.5\n",
        "        return alpha_1 * (1.0 - r) + alpha_2 * r\n",
        "\n",
        "    lr = scheduler(i / n_iters)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYoBFcwwFQKL"
      },
      "source": [
        "def set_model_FGE_modified():\n",
        "  model = set_model_FGE()\n",
        "\n",
        "  model._adjust_lr = type(model._adjust_lr)(_adjust_lr_modeified, model)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooYtP-yEFfyp"
      },
      "source": [
        "def set_model_SSE():\n",
        "  # Choose the Ensemble Method\n",
        "  model = SnapshotEnsembleClassifier(\n",
        "      estimator=resnet18(),\n",
        "      # estimator_args={\"bresnet18lock\": BasicBlock, \"num_blocks\": [2, 2, 2, 2]},\n",
        "      n_estimators=n_estimators,\n",
        "      cuda=True,\n",
        "  )\n",
        "\n",
        "  # Set the Optimizer\n",
        "  model.set_optimizer(\n",
        "      \"SGD\", lr=lr, weight_decay=weight_decay, momentum=momentum\n",
        "  )\n",
        "\n",
        "  # Set the Scheduler\n",
        "  model.set_scheduler(\"CosineAnnealingLR\", T_max=epochs)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXMjkzm4uc-D"
      },
      "source": [
        "transformer = transforms.Compose([transforms.ToTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnLCvVXEY0UB"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, average_precision_score, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guNFMeRlDoOh"
      },
      "source": [
        "# reference from https://stackoverflow.com/questions/50666091/true-positive-rate-and-false-positive-rate-tpr-fpr-for-multi-class-data-in-py\n",
        "\n",
        "def get_fpr(y_true, y_prediction):\n",
        "  cnf_matrix = confusion_matrix(y_true, y_prediction)\n",
        "\n",
        "  FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
        "  FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "  TP = np.diag(cnf_matrix)\n",
        "  TN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "\n",
        "  FP = FP.astype(float)\n",
        "  FN = FN.astype(float)\n",
        "  TP = TP.astype(float)\n",
        "  TN = TN.astype(float)\n",
        "\n",
        "  FPR = FP/(FP+TN)\n",
        "\n",
        "  return FPR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5O4roH03PYq"
      },
      "source": [
        "# modified version of evaluate function of torchensemble\n",
        "\n",
        "from torchensemble.utils.io import split_data_target\n",
        "\n",
        "def evaluate_modified(self, test_loader, return_loss=False):\n",
        "    \"\"\"Docstrings decorated by downstream models.\"\"\"\n",
        "    self.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = 0.0\n",
        "\n",
        "    ### collects all data for prediction\n",
        "    outputs = None\n",
        "    predicts = None\n",
        "    labels = None\n",
        "\n",
        "    first = True\n",
        "    start = time.time()\n",
        "    for _, elem in enumerate(test_loader):\n",
        "        data, target = split_data_target(elem, self.device)\n",
        "        output = self.forward(*data)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        total += target.size(0)\n",
        "        loss += criterion(output, target)\n",
        "\n",
        "        ### appends all data\n",
        "        if first:\n",
        "            class_num = torch.max(predicted) + 1\n",
        "            first = False\n",
        "            outputs = output\n",
        "            predicts = predicted\n",
        "            labels = target\n",
        "        else:\n",
        "            outputs = torch.cat((outputs, output))\n",
        "            predicts = torch.cat((predicts, predicted))\n",
        "            labels = torch.cat((labels, target))\n",
        "    inference_time = time.time() - start\n",
        "\n",
        "    ### calculate results\n",
        "    outputs = outputs.cpu().data.numpy()\n",
        "    outputs = outputs[:, :class_num]\n",
        "    predicts = predicts.cpu().data.numpy()\n",
        "    labels_one_hot = torch.nn.functional.one_hot(labels)#, labels.max()+1)\n",
        "    #print(labels_one_hot)\n",
        "    labels = labels.cpu().data.numpy()\n",
        "    labels_one_hot = labels_one_hot.cpu().data.numpy()\n",
        "    #print(outputs.shape, predicts.shape, labels.shape, labels_one_hot.shape)\n",
        "    result = {}\n",
        "    result['Accuracy'] = accuracy_score(labels, predicts)\n",
        "    result['TPR'] = recall_score(labels, predicts, average='macro')\n",
        "    #result['FPR'] = get_fpr(labels, predicts)\n",
        "    result['FPR'] = 1 - result['TPR']\n",
        "    result['Precision'] = precision_score(labels, predicts, average='macro')\n",
        "    result['AUC'] = roc_auc_score(labels_one_hot, outputs, multi_class='ovr')\n",
        "    result['PR-Curve'] = average_precision_score(labels_one_hot, outputs, average='macro')\n",
        "    result['Inference Time'] = inference_time / len(test_loader) * 1000\n",
        "    #print(len(test_loader))\n",
        "\n",
        "    '''\n",
        "    acc = 100 * correct / total\n",
        "    loss /= len(test_loader)\n",
        "\n",
        "    if return_loss:\n",
        "        return acc, float(loss)\n",
        "\n",
        "    return acc\n",
        "    '''\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo4sgi3ooHZo"
      },
      "source": [
        "def eval_10cv(model_type, dataset_type, dataset_split=0):\n",
        "  df = pd.DataFrame(columns=['Dataset Name', 'Algorithm Name', 'Cross Validation', 'Accuracy', 'TPR', 'FPR', 'Precision', 'AUC', 'PR-Curve', 'Training Time', 'Inference Time'])\n",
        "\n",
        "  if dataset_type == 'CIFAR10':\n",
        "    dataset = datasets.CIFAR10(data_dir, train=True, download=True, transform=transformer)\n",
        "  elif dataset_type == 'CIFAR100':\n",
        "    dataset = datasets.CIFAR100(data_dir, train=True, download=True, transform=transformer)\n",
        "  elif dataset_type == 'ImageNet':\n",
        "    dataset = datasets.ImageNet(data_dir, split='train', download=True, transform=transformer)\n",
        "  elif dataset_type == 'MNIST':\n",
        "    dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transformer)\n",
        "\n",
        "  assert 0 <= dataset_split and dataset_split <= 4\n",
        "  #split_index = np.array(range(dataset_split * len(dataset) // 4, (dataset_split+1) * len(dataset) // 4))\n",
        "  split_index = random.sample(range(len(dataset)), len(dataset))\n",
        "  split_index = split_index[dataset_split * len(dataset) // 5:(dataset_split+1) * len(dataset) // 5]\n",
        "  dataset = Subset(dataset, split_index)\n",
        "\n",
        "  kf_ex = KFold(n_splits=10)\n",
        "  all_index = range(len(dataset))\n",
        "\n",
        "  cv_num = 1\n",
        "  for train_index, test_index in kf_ex.split(all_index):\n",
        "    if model_type == 'FGE':\n",
        "      model = set_model_FGE()\n",
        "    elif model_type == 'FGEm':\n",
        "      model = set_model_FGE_modified()\n",
        "    elif model_type == 'SSE':\n",
        "      model = set_model_SSE()\n",
        "    else:\n",
        "      print('wrong argument: model_type')\n",
        "    \n",
        "    model.evaluate = evaluate_modified\n",
        "\n",
        "    train_dataset = Subset(dataset, train_index)\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "    # validation_dataset = Subset(dataset, validation_index)\n",
        "    # validation_loader = DataLoader(validation_dataset, batch_size, shuffle=True)\n",
        "    test_dataset = Subset(dataset, test_index)\n",
        "    test_loader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
        "\n",
        "    start = time.time()\n",
        "    model.fit(\n",
        "      train_loader,\n",
        "      epochs=epochs,\n",
        "      # test_loader=validation_loader,\n",
        "    )\n",
        "    train_time  = time.time() - start\n",
        "\n",
        "    result = model.evaluate(model, test_loader)\n",
        "    result['Training Time'] = train_time\n",
        "    result['Dataset Name'] = dataset_type + '_' + str(dataset_split + 1)\n",
        "    result['Algorithm Name'] = model_type\n",
        "    result['Cross Validation'] = cv_num\n",
        "    print(result)\n",
        "    df = df.append(result, ignore_index=True)\n",
        "\n",
        "    cv_num += 1\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF9jqETIg1f1"
      },
      "source": [
        "# Run with split data\n",
        "model_type = 'FGE'          # set model\n",
        "dataset_type = 'CIFAR10'    # set dataset\n",
        "data_split = 0              # set offset of the data (0-4)\n",
        "df = eval_10cv(model_type, dataset_type, data_split)\n",
        "df.to_csv(dataset_type + '_' + model_type + '_' + str(data_split) + '.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_SVDV3gVa1k"
      },
      "source": [
        "# Run with entire data\n",
        "model_type = 'FGE'          # set model\n",
        "dataset_type = 'CIFAR100'   # set dataset\n",
        "for data_split in range(5):\n",
        "  df = eval_10cv(model_type, dataset_type, data_split)\n",
        "  df.to_csv(dataset_type + '_' + model_type + '_' + str(data_split) + '.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNNBWtJRJodr"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae1ijbXBIAkr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7GMNc7h_G_"
      },
      "source": [
        "# when execution stopped, modeify this\n",
        "\n",
        "def eval_10cv(model_type, dataset_type, dataset_split=0):\n",
        "  df = pd.DataFrame(columns=['Dataset Name', 'Algorithm Name', 'Cross Validation', 'Accuracy', 'TPR', 'FPR', 'Precision', 'AUC', 'PR-Curve', 'Training Time', 'Inference Time'])\n",
        "\n",
        "  if dataset_type == 'CIFAR10':\n",
        "    dataset = datasets.CIFAR10(data_dir, train=True, download=True, transform=transformer)\n",
        "  elif dataset_type == 'CIFAR100':\n",
        "    dataset = datasets.CIFAR100(data_dir, train=True, download=True, transform=transformer)\n",
        "  elif dataset_type == 'ImageNet':\n",
        "    dataset = datasets.ImageNet(data_dir, split='train', download=True, transform=transformer)\n",
        "  elif dataset_type == 'MNIST':\n",
        "    dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transformer)\n",
        "\n",
        "  assert 0 <= dataset_split and dataset_split <= 4\n",
        "  #split_index = np.array(range(dataset_split * len(dataset) // 4, (dataset_split+1) * len(dataset) // 4))\n",
        "  split_index = random.sample(range(len(dataset)), len(dataset))\n",
        "  split_index = split_index[dataset_split * len(dataset) // 5:(dataset_split+1) * len(dataset) // 5]\n",
        "  dataset = Subset(dataset, split_index)\n",
        "\n",
        "  kf_ex = KFold(n_splits=10)\n",
        "  all_index = range(len(dataset))\n",
        "\n",
        "  cv_num = 1\n",
        "  for train_index, test_index in kf_ex.split(all_index):\n",
        "    if cv_num <= 7:\n",
        "      rlist = [{'Accuracy': 0.498, 'TPR': 0.5015319855358373, 'FPR': 0.49846801446416267, 'Precision': 0.5000659568832656, 'AUC': 0.8892649826662705, 'PR-Curve': 0.546320074481208, 'Inference Time': 274.2760181427002, 'Training Time': 1043.5634968280792, 'Dataset Name': 'CIFAR10_4', 'Algorithm Name': 'FGEm', 'Cross Validation': 1},\n",
        "               {'Accuracy': 0.556, 'TPR': 0.5575471448862332, 'FPR': 0.44245285511376675, 'Precision': 0.5598966906084876, 'AUC': 0.9071498228363295, 'PR-Curve': 0.5964666483979707, 'Inference Time': 270.8180546760559, 'Training Time': 1044.0145936012268, 'Dataset Name': 'CIFAR10_4', 'Algorithm Name': 'FGEm', 'Cross Validation': 2},\n",
        "               {'Accuracy': 0.547, 'TPR': 0.5429857096417257, 'FPR': 0.45701429035827434, 'Precision': 0.5431857477841769, 'AUC': 0.9016243399998078, 'PR-Curve': 0.583741794720911, 'Inference Time': 268.0376172065735, 'Training Time': 1042.560531616211, 'Dataset Name': 'CIFAR10_4', 'Algorithm Name': 'FGEm', 'Cross Validation': 3},\n",
        "               {'Accuracy': 0.524, 'TPR': 0.5229544236788614, 'FPR': 0.47704557632113864, 'Precision': 0.5291013122659229, 'AUC': 0.8895413867011539, 'PR-Curve': 0.5628302842973161, 'Inference Time': 272.3956108093262, 'Training Time': 1043.731341123581, 'Dataset Name': 'CIFAR10_4', 'Algorithm Name': 'FGEm', 'Cross Validation': 4},\n",
        "               {'Accuracy': 0.573, 'TPR': 0.5691689652587358, 'FPR': 0.43083103474126416, 'Precision': 0.5717630822138045, 'AUC': 0.9132241303668558, 'PR-Curve': 0.6119640876794608, 'Inference Time': 268.7094211578369, 'Training Time': 1043.161875963211, 'Dataset Name': 'CIFAR10_4', 'Algorithm Name': 'FGEm', 'Cross Validation': 5},\n",
        "               {'Accuracy': 0.543, 'TPR': 0.5462239083448645, 'FPR': 0.45377609165513555, 'Precision': 0.5413901388199379, 'AUC': 0.895189039275121, 'PR-Curve': 0.574292156360536, 'Inference Time': 271.90470695495605, 'Training Time': 1044.248327255249, 'Dataset Name': 'CIFAR10_4', 'Algorithm Name': 'FGEm', 'Cross Validation': 6},\n",
        "               {'Accuracy': 0.553, 'TPR': 0.5596245081520613, 'FPR': 0.44037549184793867, 'Precision': 0.5554395041994891, 'AUC': 0.9005340104482151, 'PR-Curve': 0.5809220223042436, 'Inference Time': 269.21379566192627, 'Training Time': 1040.0652377605438, 'Dataset Name': 'CIFAR10_4', 'Algorithm Name': 'FGEm', 'Cross Validation': 7}\n",
        "\n",
        "      ]\n",
        "      df = df.append(rlist[cv_num - 1], ignore_index=True)\n",
        "      print('check:', rlist[cv_num - 1])\n",
        "      cv_num += 1\n",
        "      continue\n",
        "\n",
        "\n",
        "    if model_type == 'FGE':\n",
        "      model = set_model_FGE()\n",
        "    elif model_type == 'FGEm':\n",
        "      model = set_model_FGE_modified()\n",
        "    elif model_type == 'SSE':\n",
        "      model = set_model_SSE()\n",
        "    else:\n",
        "      print('wrong argument: model_type')\n",
        "    \n",
        "    model.evaluate = evaluate_modified\n",
        "\n",
        "    train_dataset = Subset(dataset, train_index)\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "    # validation_dataset = Subset(dataset, validation_index)\n",
        "    # validation_loader = DataLoader(validation_dataset, batch_size, shuffle=True)\n",
        "    test_dataset = Subset(dataset, test_index)\n",
        "    test_loader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
        "\n",
        "    start = time.time()\n",
        "    model.fit(\n",
        "      train_loader,\n",
        "      epochs=epochs,\n",
        "      # test_loader=validation_loader,\n",
        "    )\n",
        "    train_time  = time.time() - start\n",
        "\n",
        "    result = model.evaluate(model, test_loader)\n",
        "    result['Training Time'] = train_time\n",
        "    result['Dataset Name'] = dataset_type + '_' + str(dataset_split + 1)\n",
        "    result['Algorithm Name'] = model_type\n",
        "    result['Cross Validation'] = cv_num\n",
        "    print(result)\n",
        "    df = df.append(result, ignore_index=True)\n",
        "\n",
        "    cv_num += 1\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}